{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f:\\\\PROJECTS\\\\imdb 50k NLP\\\\Sentiment-analysis-mlflow\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f:\\\\PROJECTS\\\\imdb 50k NLP\\\\Sentiment-analysis-mlflow'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['MLFLOW_TRACKING_URI']= 'https://dagshub.com/chrisaaryan/my-first-repo.mlflow'\n",
    "os.environ['MLFLOW_TRACKING_USERNAME']= 'chrisaaryan'\n",
    "os.environ['MLFLOW_TRACKING_PASSWORD']= '97f85b6ca1fc224edc2875d929cf45a081ca85b1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model= tf.keras.models.load_model(\"artifacts/model/model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class EvaluationConfig:\n",
    "    path_of_model: Path\n",
    "    test_data: Path\n",
    "    all_params: dict\n",
    "    mlflow_uri: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sentimentAnalysis.constants import *\n",
    "from sentimentAnalysis.utils.common import read_yaml, create_directories\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(self, config_filepath=CONFIG_FILE_PATH, params_filepath=PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_evaluation_config(self) -> EvaluationConfig:\n",
    "        eval_config = EvaluationConfig(\n",
    "            path_of_model=Path(self.params['model_config']['save_model_path']),  # Path where the model is stored\n",
    "            test_data=Path(self.config['data_preprocessing']['test_data_path']),  # Path to test_data.csv\n",
    "            mlflow_uri=self.config['mlflow_config']['mlflow_uri'],  # MLflow URI\n",
    "            all_params=self.params  # Other parameters like max sequence length, etc.\n",
    "        )\n",
    "        return eval_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.keras\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "from sentimentAnalysis.utils.common import save_json\n",
    "\n",
    "class Evaluation:\n",
    "    def __init__(self, config: EvaluationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    @staticmethod\n",
    "    def load_model(path: Path) -> tf.keras.Model:\n",
    "        return tf.keras.models.load_model(path)\n",
    "    \n",
    "    def _load_test_data(self):\n",
    "        # Load test data (assuming 'preprocessed_review' is already tokenized and padded)\n",
    "        df = pd.read_csv(self.config.test_data)\n",
    "        X_test = df['preprocessed_review'].values  # Use the preprocessed text\n",
    "        Y_test = df['sentiment'].values  # Labels (either binary or categorical)\n",
    "        return X_test, Y_test\n",
    "    \n",
    "    def _evaluate_model(self, X_test, Y_test):\n",
    "        # Use the tokenizer saved during training if applicable\n",
    "        tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=self.config.all_params['model_config']['input_dim'])\n",
    "        # Assume tokenizer has been fit on training data\n",
    "        X_test_seq = tokenizer.texts_to_sequences(X_test)  # Tokenize test data\n",
    "        X_test_pad = tf.keras.preprocessing.sequence.pad_sequences(X_test_seq, maxlen=self.config.all_params['model_config']['input_length'])\n",
    "\n",
    "        # Perform predictions\n",
    "        predictions = self.model.predict(X_test_pad)\n",
    "        predictions = (predictions > 0.5).astype(int)  # Assuming binary classification\n",
    "\n",
    "        # Calculate evaluation metrics\n",
    "        accuracy = accuracy_score(Y_test, predictions)\n",
    "        precision = precision_score(Y_test, predictions)\n",
    "        recall = recall_score(Y_test, predictions)\n",
    "        f1 = f1_score(Y_test, predictions)\n",
    "\n",
    "        return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1_score\": f1}\n",
    "\n",
    "    def evaluation(self):\n",
    "        self.model = self.load_model(self.config.path_of_model)\n",
    "        X_test, Y_test = self._load_test_data()\n",
    "        self.metrics = self._evaluate_model(X_test, Y_test)\n",
    "        self.save_score()\n",
    "\n",
    "    def save_score(self):\n",
    "        # Save metrics as JSON\n",
    "        save_json(path=Path(\"evaluation_scores.json\"), data=self.metrics)\n",
    "\n",
    "    def log_into_mlflow(self):\n",
    "        # Set MLflow tracking URI\n",
    "        mlflow.set_registry_uri(self.config.mlflow_uri)\n",
    "        tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "\n",
    "        # Start logging with MLflow\n",
    "        with mlflow.start_run():\n",
    "            # Log parameters and metrics\n",
    "            mlflow.log_params(self.config.all_params)\n",
    "            mlflow.log_metrics(self.metrics)\n",
    "\n",
    "            # Log the model to MLflow\n",
    "            if tracking_url_type_store != \"file\":\n",
    "                mlflow.keras.log_model(self.model, \"model\", registered_model_name=\"SentimentAnalysisModel2\")\n",
    "            else:\n",
    "                mlflow.keras.log_model(self.model, \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 28ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/09/27 12:53:15 WARNING mlflow.keras.save: You are saving a Keras model without specifying model signature.\n",
      "2024/09/27 12:53:25 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "Registered model 'SentimentAnalysisModel2' already exists. Creating a new version of this model...\n",
      "2024/09/27 12:53:33 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: SentimentAnalysisModel2, version 2\n",
      "Created version '2' of model 'SentimentAnalysisModel2'.\n",
      "2024/09/27 12:53:34 INFO mlflow.tracking._tracking_service.client: üèÉ View run judicious-ray-483 at: https://dagshub.com/chrisaaryan/my-first-repo.mlflow/#/experiments/0/runs/bb7b3efcf54a4f82850cd9bf11a3862a.\n",
      "2024/09/27 12:53:34 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: https://dagshub.com/chrisaaryan/my-first-repo.mlflow/#/experiments/0.\n"
     ]
    }
   ],
   "source": [
    "from sentimentAnalysis import logger\n",
    "\n",
    "try:\n",
    "    logger.info(\"Starting model evaluation...\")\n",
    "\n",
    "    # Configuration Manager\n",
    "    config = ConfigurationManager()\n",
    "    eval_config = config.get_evaluation_config()\n",
    "\n",
    "    # Model Evaluation\n",
    "    evaluation = Evaluation(eval_config)\n",
    "    evaluation.evaluation()  # Evaluate the model\n",
    "    evaluation.log_into_mlflow()  # Log metrics and model to MLflow\n",
    "\n",
    "    logger.info(\"Model evaluation and logging completed.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.exception(f\"Error occurred during model evaluation: {e}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
